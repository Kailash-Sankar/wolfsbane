
{{{
  "title": "Knowledge Stream #6",
  "tags": ["UI","Science"],
  "category": "Knowledge Streams",
  "date": "15 April 2016",
  "preview": "This knowledge stream looks at virtual reality, neuromorphic computing, bitcoins, CRISPR/Cas9, whaling attack and AngularJS."
}}}


#### Virtual Reality

Virtual reality is an artificial environment that is created with software and presented to the user in such a way that the user suspends belief and accepts it as a real environment. On a computer, virtual reality is primarily experienced through sight and sound. Scientists, theorists and engineers have designed dozens of devices and applications to achieve this goal.  Opinions differ on what exactly constitutes a true VR experience, but in general it should include:
- Three-dimensional images that appear to be life-sized from the perspective of the user
- The ability to track a user's motions, particularly his head and eye movements, and correspondingly adjust the images on  the user's display to reflect the change in perspective

###### Characteristics of a VR system

_Immersion:_ In a VR Environment the user feels part of that world and is able to interact with his environment, the combination of this sense of immersion and interactivity is called telepresence. Computer scientist Jonathan Steuer defined it as “the extent to which one feels present in the mediated environment, rather than in the immediate physical environment.”

Two main components of immersion are depth of information and breadth of information. Depth of information refers to the amount and quality of data in the signals a user receives when interacting in a virtual environment. For the user, this could refer to a display’s resolution, the complexity of the environment’s graphics, the sophistication of the system’s audio output, etc.  Breadth of information is the number of sensory dimensions simultaneously presented. Most virtual environment experiences prioritize visual and audio components over other sensory-stimulating factors, but a growing number of scientists and engineers are looking into ways to incorporate a users’ sense of touch (Haptic systems).

_Environment:_ Output from the VE system should adjust in real time as a user explores the environment. If the environment incorporates 3-D sound, the user must be convinced that the sound’s orientation shifts in a natural way as he maneuvers through the environment. Lag time between when a user acts and when the virtual environment reflects that action is called latency. Simulators show that humans can detect a latency of more than 50 milliseconds. When a user detects latency, it causes him to become aware of being in an artificial environment and destroys the sense of immersion.

_Interactivity:_ Interactivity depends on these three factors - speed, range and mapping.
Speed as the rate that a user's actions are incorporated into the computer model and reflected in a way the user can perceive.
Range refers to how many possible outcomes could result from any particular user action.
Mapping is the system's ability to produce natural results in response to a user's actions.

Navigation within a virtual environment is one kind of interactivity. If a user can direct his own movement within the environment, it can be called an interactive experience. True interactivity also includes being able to modify the environment. A good virtual environment will respond to the user's actions in a way that makes sense, even if it only makes sense within the realm of the virtual environment. If a virtual environment changes in outlandish and unpredictable ways, it risks disrupting the user's sense of telepresence.


###### VR Systems

VE systems mainly use head mounted displays(HMDs) to display images to a user. HMDs are headsets that contain two monitors, one for each eye. The images create a stereoscopic effect, giving the illusion of depth. There's also a new VR headset in development which projects images directly into the eye from the headset. Other VE systems project images on the walls, floor and ceiling of a room and are called Cave Automatic Virtual Environments (CAVE). The University of Illinois-Chicago designed the first CAVE display, using a rear projection technique to display images on the walls, floor and ceiling of a small room. CAVE displays are very expensive and require more space than other systems.

Closely related to display technology are tracking systems. Tracking systems analyze the orientation of a user’s point of view so that the computer system sends the right images to the visual display. Most systems require a user to be tethered with cables to a processing unit, limiting the range of motions available to him.

Input devices are also important in VR systems. Currently, input devices range from controllers to electronic gloves and voice recognition software. There is no standard control system across the discipline. VR scientists and engineers are continuously exploring ways to make user input as natural as possible to increase the sense of telepresence.

Scientists are also exploring the possibility of developing biosensors for VR use. A biosensor can detect and interpret nerve and muscle activity. With a properly calibrated biosensor, a computer can interpret how a user is moving in physical space and translate that into the corresponding motions in virtual space.

###### Major Applications of VR

_Gaming:_ Gaming industry already has an array of VR ready titles.  I’m afraid of how many accidents this could lead to.

_Design:_ Some architects create virtual models of their building plans so that people can walk through the structure before the foundation is even laid. Clients can move around exteriors and interiors and ask questions, or even suggest alterations to the design. Virtual models can give you a much more accurate idea of how moving through a building will feel than a miniature model. Car companies have used VR technology to build virtual prototypes of new vehicles, testing them thoroughly before producing a single physical part.

_Military:_  The military have long been supporters of VR technology and development. Training programs can include everything from vehicle simulations to squad combat. On the whole, VR systems are much safer and, in the long run, less expensive than alternative training methods. Soldiers who have gone through extensive VR training have proven to be as effective as those who trained under traditional conditions.

_Medicine:_ Staff can use virtual environments to train in everything from surgical procedures to diagnosing a patient. Surgeons have used virtual reality technology to not only train and educate, but also to perform surgery remotely by using robotic devices. Another medical use of VR technology is psychological therapy. Dr. Barbara Rothbaum of Emory University and Dr. Larry Hodges of the Georgia Institute of Technology pioneered the use of virtual environments in treating people with phobias and other psychological conditions.

###### Most popular VR  Gear

_HTC Vive:_  Vive is a Steam VR headset made in collaboration with Valve. It packs in 70 sensors plus offers 360 degree head-tracking as well as a 90Hz refresh rate; the stat that's key to keeping down latency. Elsewhere, there's an accompanying "context aware controller", so you can shoot, move and interact with elements in the virtual world.

_Oculus Rift:_ Developed by Palmer Luckey, funded via Kickstarter and snapped up by Facebook for a $2 billion, the Rift plugs into your computer's DVI and USB ports and tracks your head movements to provide 3D imagery on its stereo screens.

_Microsoft HoloLens:_  Microsoft HoloLens is half virtual and half augmented reality. The device merges real-world elements with virtual 'holographic' images, meaning you can look at your Minecraft world on your kitchen table, or walk around the surface of Mars in your living room. Using Kinect-style tech to recognize gestures and voice commands, the headset has a 120-degree field of vision on both axes, and is capable of 'high definition' visuals. What's more, there's no connection to a PC – a full Windows 10 system is built into the headset and runs off a battery.

_Avegant Glyph:_ Avegant's Glyph is both sleeker and smaller due to its display technology: rather than using conventional smartphone-like screens to present imagery, it uses an array of micro mirrors to reflect an image directly into your retina. The Glyph can be worn like a pair of headphones until you pull the screen down over your eyes, where you can enjoy 1280 x 720 for each eyeball. And while it's limited to a 45 degree field of view, the micro mirror array is said to reduce motion sickness and eye fatigue.

Sources: [howstuffworks](http://electronics.howstuffworks.com/gadgets/other-gadgets/virtual-reality.htm), [vrs](http://www.vrs.org.uk/virtual-reality-applications), [wareable](http://www.wareable.com/headgear/the-best-ar-and-vr-headsets)


#### Neuromorphic computing

Modern compute power has enabled neural networks to recognize images, speech, and faces, as well as to pilot self-driving cars, and win at Go and Jeopardy. Unfortunately, the hardware we use to train and run neural networks looks almost nothing like their architecture. That means it can take days or even weeks to train a neural network to solve a problem — even on a compute cluster — and then require a large amount of power to solve the problem once they’re trained.

Researchers at IBM aim to change all that, by perfecting another technology like neural networks called resistive computing. The concept is to have compute units that are analog in nature, small in substance, and can retain their history so they can learn during the training process. IBM has already made progress in this area, IBMs 'TrueNorth' chips design is neuromorphic, meaning that the chips roughly approximate the brain’s architecture of neurons and synapses. Despite its slow clock rate of 1 KHz, TrueNorth can run neural networks very efficiently because of its million tiny processing units that each emulate a neuron.

Until now, though, neural network accelerators like TrueNorth have been limited to the problem-solving portion of deploying a neural network. Training — the painstaking process of letting the system grade itself on a test data set, and then tweaking parameters (called weights) until it achieves success — still needs to be done on traditional computers. Moving from CPUs to GPUs and custom silicon has increased performance and reduced the power consumption required, but the process is still expensive and time consuming. That is where new work by IBM researchers Tayfun Gokmen and Yuri Vlasov comes in. They propose a new chip architecture, using resistive computing to create tiles of millions of Resistive Processing Units (RPUs), which can be used for both training and running neural networks.

In the IBM design each small processing unit (RPU) mimics a synapse in the brain. It receives a variety of analog inputs — in the form of voltages — and based on its past “experience” uses a weighted function of them to decide what result to pass along to the next set of compute elements. Synapses have a bewildering, and not-yet totally understood layout in the brain, but chips with resistive elements tend to have them neatly organized in two-dimensional arrays. For example, IBM’s recent work shows how it is possible to organize them in 4,096-by-4,096 arrays. In theory, a complex neural network — like the ones used to recognize road signs in a self-driving car, for example — can be directly modeled by dedicating a resistive compute element to each of the software-described nodes. RPUs are imprecise — due to their analog nature and a certain amount of noise in their circuitry — any algorithm run on them needs to be made resistant to the imprecision inherent in resistive computing elements.

This new work from IBM postulates a system where almost all computation is done on RPUs, with traditional circuitry only needed for support functions and input and output. This innovation relies on combining a version of a neural network training algorithm that can run on an RPU-based architecture with a hardware specification for an RPU that could run it. Using existing CMOS technology, and assuming RPUs in 4,096-by-4,096-element tiles with an 80-nanosecond cycle time, one of these tiles would be able to execute about 51 GigaOps per second, using a minuscule amount of power. A chip with 100 tiles and a single complementary CPU core could handle a network with up to 16 billion weights while consuming only 22 watts. Researchers claim that, once built, a resistive-computing-based AI system can achieve performance improvements of up to 30,000 times compared with current architectures, all with a power efficiency of 84,000 GigaOps per-second per-watt.

Sources: [extremetech](http://www.extremetech.com/extreme/225707-ibms-resistive-computing-could-massively-accelerate-ai-and-get-us-closer-to-asimovs-positronic-brain), [ibm](http://www.research.ibm.com/articles/brain-chip.shtml)


#### Microsoft: Embracing open source (Not an April fools joke)

_Eclipse:_ Microsoft is making eclipse a full-fledged option for those developing for Microsoft’s Azure services or using Visual Studio’s team development features. Microsoft has been shipping a number of Eclipse-based tools already, including an Azure Toolkit for Eclipse and a Java SDK for Azure. Its ‘Team Explorer Everywhere’ already allows Eclipse developers to access Visual Studio’s Team Services from within Eclipse — build, test, and source code control, for example.
Microsoft embracing the Eclipse IDE is very much in line with Microsoft’s newish “embrace and extend” approach to other popular tools and environments. Ironically, perhaps, at the same time Google has moved to distance itself from Eclipse, by ending support for its Android development plug-in for Eclipse in favor of its own Android Studio offering.

_SQL Server for Linux:_  Microsoft announced that an edition of its SQL Server software for Linux will arrive in mid-2017. “We are delighted to be working with Microsoft as it brings SQL Server to Linux,” said Mark Shuttleworth, founder of Canonical (The Ubuntu Guy).
According to Microsoft’s blog post, SQL Server on Linux will provide customers with even more flexibility in their data solution. One with mission-critical performance, industry-leading TCO, best-in-class security, and hybrid cloud innovations – like Stretch Database which lets customers access their data on-premises and in the cloud whenever they want at low cost – all built in.

_Bash for Windows 10:_  Windows 10 Anniversary Update will include the ability to run the popular bash shell from Unix, along with the rest of a typical Unix command-line environment. "This is not a VM. This is not cross-compiled tools. This is native," said Microsoft's Kevin Gallo.

2016 is turning out to be a good year for developers.

Sources: [arstechnica](http://arstechnica.com/information-technology/2016/03/ubuntus-bash-and-linux-command-line-coming-to-windows-10/), [extremetech](http://www.extremetech.com/computing/224510-microsoft-reaches-out-and-embraces-open-source-eclipse),
[extremetech](http://www.extremetech.com/computing/224309-microsoft-announces-sql-server-for-linux-wait-what)


#### Bitcoin (Part 2): The Rise and Fall

Bitcoin started small, as an open source experiment. The small band of early bitcoiners all shared the communitarian spirit of an open source software project. Gavin Andresen, a coder in New England, bought 10,000 bitcoins for $50 and created a site called the Bitcoin Faucet, where he gave them away. Laszlo Hanyecz, a Florida programmer, conducted what bitcoiners think of as the first real-world bitcoin transaction, paying 10,000 bitcoins to get two pizzas delivered from Papa John’s. A farmer in Massachusetts named David Forster began accepting bitcoins as payment for alpaca socks.

###### Bitcoin Gold Rush

Through 2009 and early 2010, bitcoins had no value at all, and for the first six months after they started trading the value of one bitcoin stayed below 14 cents. Then, as the currency gained viral traction in summer 2010, rising demand for a limited supply caused the price on online exchanges to start moving. Thanks to stories published by Forbes, Slashdot and Gawker the value of bitcoin rose to $27 by end of 2011. Much of bitcoins popularity was attributed to its use in online drug markets like Silk Road as the coins cannot be traced back to its user.

As the price rose and mining became more popular, the increased competition meant decreasing profits. Miners looking for horsepower supplemented their computers with more powerful graphics cards, bought racks of cheap computer with high end GPU's. The boom gave rise to mining specific hardware like these - [asrock](http://www.asrock.com/microsite/mining/index.en.html)

Entrepreneurs began setting up mining farms and leasing out hardware. The hardware industry profited greatly from the rush - [businessinsider](http://www.businessinsider.in/These-Guys-Made-3-Million-In-Four-Days-From-The-Bitcoin-Craze/articleshow/25805605.cms)

The price of one bitcoin surged *78-fold ($1,044) in 2013* on hopes the experiment in digital money will eventually become a legitimate global currency.

###### A Series of Unfortunate Events

The popularity of Silk Road( Amazon for drugs) brought all kind of heat to bitcoin. US senator Charles Schumer held a press conference, appealing to the DEA and Justice Department to shut down Silk Road, which he called “the most brazen attempt to peddle drugs online that we have ever seen” and describing bitcoin as “an online form of money-laundering.”

Bitcoins became a target for hackers. Bitcoins were stolen from computers, bitcoin exchanges were hacked and security researchers detected a proliferation of viruses aimed at bitcoin users: Some were designed to steal wallets full of existing bitcoins; others commandeered processing power to mine fresh coins.
These are the **greatest bitcoin heists**:
1. Mt. Gox ( ~ $436 million ) - http://www.csmonitor.com/Business/Latest-News-Wires/2014/0225/Bitcoin-heist-darkens-Mt.-Gox-exchange-massive-theft-reported
2. Silk Road ( ~ $127.4 million now ) - http://gizmodo.com/the-fbi-just-scooped-up-28-5-million-in-bitcoin-from-s-1452328051
3. Sheep Marketplace( $56.4 Million ) - http://www.newstatesman.com/future-proof/2013/12/theres-%C2%A360m-bitcoin-heist-going-down-right-now-and-you-can-watch-real-time

The increasing processing requirements led to the formation of various mining groups and the whole bitcoin mining operation was divided among a few top players. At one point of time, GHash.io, a Ukraine-based collective of Bitcoin miners controlled 42 percent of all the world’s ability to create bitcoins. This brought fear over something called a “51 percent attack.” An entity that controls over half of the processing power on the entire Bitcoin network can use that power to engage in a whole mess of shenanigans, like getting away with spending a single Bitcoin an infinite number of times, or preventing other Bitcoin users’ transactions from going through. At 42 percent, GHash was short of the full amount it would need to cause serious trouble, and the group has since taken steps to drop its processing power down to an even less threatening level.

Most significantly, these incidents had shaken the confidence of the community and inspired loads of bad press and bitcoin value fell mostly through 2014 and 2015.

###### The Mystery Man

Satoshi Nakamoto was the inventor of the bitcoin protocol, publishing a paper via the Cryptography Mailing List in November 2008. Nakamoto worked with people on the open-source team, but took care never to reveal anything personal about himself, and the last anyone heard from him was in the spring of 2011, when he said that he had “moved on to other things”. We can’t know for sure whether he/she was Japanese or not.

Nobody knows who Nakamoto is but there have been many calculated guesses. Quoting a few -
The New Yorker’s Joshua Davis believed that Satoshi Nakamoto was Michael Clear, a graduate cryptography student at Dublin's Trinity College. He arrived at this conclusion by analyzing 80,000 words of Nakamoto’s online writings, and searching for linguistic clues.
Adam Penenberg at FastCompany disputed that claim, arguing instead that Nakamoto may actually have been three people: Neal King, Vladimir Oksman, and Charles Bry. He figured this out by typing unique phrases from Nakamoto’s bitcoin paper into Google, to see if they were used anywhere else.
In February 2014, Newsweek’s Leah McGrath Goodman claimed to have tracked down the real Satoshi Nakamoto. Dorian S Nakamoto has since denied he knows anything about bitcoin, eventually hiring a lawyer and releasing an official statement to that effect.

It is believed that Satoshi mined many of the early blocks in the bitcoin network, and that he had built up a fortune of around 1 million unspent bitcoins. That hoard would be worth $1bn at November 2013’s exchange rate of $1,000.

The fact that the original author is nowhere to be found has resulted in too many accusations and wild theories,  including a claim that he worked for the Government.


###### A Failed Experiment

Mike Hearn, one of the early members of the Bitcoin core developer team, published a blog post early this year stating that “Bitcoin Has Failed”. When one of the most important people in Bitcoin states something like that, it should be taken seriously. According to Mike, Bitcoin failed because the community has failed. What was meant to be a new, decentralized form of money that lacked “systemically important institutions” and “too big to fail” has become something even worse: a system completely controlled by just a handful of people. Worse still, the network is on the brink of technical collapse. The mechanisms that should have prevented this outcome have broken down, and as a result there’s no longer much reason to think Bitcoin can actually be better than the existing financial system.

To sum up, this is the current state of Bitcoin:
It’s difficult to move your existing money
wildly unpredictable fees
large backlogs and flaky payments
controlled by China
companies and people building it are in open civil war

There's also a serious technical issue. The block chain is full. An entirely artificial capacity cap of one megabyte per block was put in place as a temporary kludge a long time ago, has not been removed and as a result the network’s capacity is now almost completely exhausted. When networks run out of capacity, they get really unreliable. It’s now officially impossible to depend upon the bitcoin network anymore to know when or if your payment will be transacted, because the congestion is so bad that even minor spikes in volume create dramatic changes in network conditions.

The capacity limit is not being increased because the block chain is controlled by Chinese miners, just two of whom control more than 50% of the hash power. The miners are not allowing the block chain to grow. The Chinese internet is so broken by their government’s firewall that moving data across the border barely works at all, with speeds routinely worse than what mobile phones provide. Right now, the Chinese miners are able to just about maintain their connection to the global internet and claim the 25 BTC reward ($11,000) that each block they create gives them. But if the Bitcoin network got more popular, they fear taking part would get too difficult and they’d lose their income stream. This gives them a perverse financial incentive to actually try and stop Bitcoin becoming popular.

The resulting civil war has seen Coinbase the largest and best known Bitcoin startup in the USA be erased from the official Bitcoin website for picking the wrong side and banned from the community forums. When parts of the community are viciously turning on the people that have introduced millions of users to the currency, you know things have got really crazy.

Bitcoin is not intended to be an investment and has always been advertised pretty accurately: as an experimental currency which you shouldn’t buy more of than you can afford to lose.

Sources: [coindesk](http://www.coindesk.com/information/who-is-satoshi-nakamoto/), [medium](https://medium.com/@octskyward/the-resolution-of-the-bitcoin-experiment-dabb30201f7#.nmyb45tcy), [avc](http://avc.com/2016/01/bitcoin-is-dead-long-live-bitcoin/)


#### CRISPR/Cas9: Gene editing

A revolution has seized the scientific community. Within only a few years, research labs worldwide have adopted a new technology that facilitates making specific changes in the DNA of humans, other animals, and plants. Compared to previous techniques for modifying DNA, this new approach is much faster and easier. This technology is referred to as “CRISPR" (Clustered Regularly Interspaced Short Palindromic Repeat.)

CRISPR allows scientists to edit genomes with unprecedented precision, efficiency, and flexibility. The past few years have seen a flurry of “firsts” with CRISPR, from creating monkeys with targeted mutations to preventing HIV infection in human cells. Chinese scientists announced they applied the technique to nonviable human embryos, hinting at CRISPR’s potential to cure any genetic disease.

The name refers to the unique organization of short, partially palindromic repeated DNA sequences found in the genomes of bacteria and other microorganisms. While seemingly innocuous, CRISPR sequences are a crucial component of the immune systems of these simple life forms. If a viral infection threatens a bacterial cell, the CRISPR immune system can thwart the attack by destroying the genome of the invading virus. The genome of the virus includes genetic material that is necessary for the virus to continue replicating. Thus, by destroying the viral genome, the CRISPR immune system protects bacteria from ongoing viral infection.

The inherent functions of the CRISPR system are advantageous for industrial processes that utilize bacterial cultures. CRISPR-based immunity can be employed to make these cultures more resistant to viral attack, which would otherwise impede productivity. Beyond applications encompassing bacterial immune defenses, scientists have learned how to harness CRISPR technology in the lab to make precise changes in the genes of organisms as diverse as fruit flies, fish, mice, plants and even human cells.

**Permanently Inactivate HIV-1:** Last month, researchers in the Lewis Katz School of Medicine at Temple University published a ground breaking HIV genome eradication study using the CRISPR/Cas9 technology. The study, conducted using a variety of cells including cells isolated from HIV-1+ patient showed the introduction of mutations within the targeted viral genes by CRISPR/Cas9 with no off-target effects on the host genome. Researchers were able to permanently Inactivate HIV-1 in Patient's Blood for First Time.
[prnewswire](http://www.prnewswire.com/news-releases/use-of-crispr-cas9-gene-editing-therapeutic-shown-to-permanently-inactivate-hiv-1-in-patients-blood-for-first-time-300239112.html#continue-jump)

**Malaria-Blocking Mosquitoes:** Biologists at UC San Diego, working in collaboration with biologists at UC Irvine, have created a strain of mosquitoes capable of rapidly introducing malaria-blocking genes into a mosquito population through its progeny, ultimately eliminating the insects’ ability to transmit the disease to humans.
[ucsd](http://ucsdnews.ucsd.edu/pressrelease/biologists_create_malaria_blocking_mosquitoes)

**Gene-Edited Dogs:** Scientists in China say they are the first to use gene editing to produce customized dogs. They created a beagle with double the amount of muscle mass by deleting a gene called myostatin. The dogs have “more muscles and are expected to have stronger running ability, which is good for hunting, police (military) applications, said Liangxue Lai, a researcher with the Key Laboratory of Regenerative Biology at the Guangzhou Institutes of Biomedicine and Health.
[technologyreview](https://www.technologyreview.com/s/542616/first-gene-edited-dogs-reported-in-china/)

**Gene-Edited 'Micropigs':** BGI in Shenzhen, the genomics institute that is famous for a series of high-profile breakthroughs in genomic sequencing, originally created the micropigs as models for human disease, by applying gene-editing technique to a small breed of pig known as Bama. BGI revealed at the Shenzhen International Biotech Leaders’ Summit in China, that it would start selling the pigs as pets. The animals weigh about 15 kilograms when mature, or about the same as a medium-sized dog.
[nature](http://www.nature.com/news/gene-edited-micropigs-to-be-sold-as-pets-at-chinese-institute-1.18448)

**Bringing back the Mammoth:**  Genetic engineer George Church is trying to use this technique to bring back the mammoth. By splicing genes responsible for traits like thicker hair, subcutaneous fat and curving tusks into the DNA of an Asian elephant, Church hopes to revive the long-extinct woolly mammoth, or at least create a version of the modern elephant that really likes the cold.
[recode](http://recode.net/2014/12/08/the-time-traveler-george-church-is-racing-into-the-future-and-reaching-into-the-past/)


Sources: [gizmodo](http://gizmodo.com/everything-you-need-to-know-about-crispr-the-new-tool-1702114381), [motherboard](http://motherboard.vice.com/read/what-is-crisprcas9-and-why-is-it-suddenly-everywhere)


#### Whaling Attack

Phishing is the practice of sending email to users with the purpose of tricking them into clicking on a link or revealing personal information. Spear phishing and whaling are targeted phishing attacks.

Phishing emails go to a wide group of people without targeting anyone. It’s like a fisherman casting a wide net trying to see what he can catch. The attackers know that not everyone will respond, but they know that if they send enough emails out, enough people will respond. Spear phishing targets a group of people. For example, a spear phishing email can target employees of a specific company, customers of a specific company, or even a specific person.

Vishing is a form of phishing that uses the phone system or voice over IP (VoIP) technologies. The user may receive an email, a phone message, or even a text encouraging them to call a phone number due to some discrepancy. If they call, an automated recording prompts them to provide detailed information to verify their account such as credit card number, expiration date, birthdate, and so on.

Whaling is a type of fraud that targets high-profile end users such as C-level corporate executives, politicians and celebrities. The goal of whaling is to trick someone into disclosing personal or corporate information through social engineering, email spoofing and content spoofing efforts. The attacker may send his target an email that appears as if it's from a trusted source or lure the target to a website that has been created especially for the attack. Whaling emails and websites are highly customized and personalized, often incorporating the target's name, job title or other relevant information gleaned from a variety of sources.

Whaling scam emails are designed to masquerade as a critical business email, sent from a legitimate business authority. The content is meant to be tailored for upper management, and usually involves some kind of falsified company-wide concern. In whaling attacks, big phish are carefully chosen because of their position within the organization. The number of emails distributed is very small compared to a massive phishing campaign that might involve hundreds, thousands, tens of thousands or more e-mails being sent. Due to their focused nature, whaling attacks are often harder to detect than standard phishing attacks.

The foundation for a successful whaling attack is information about the intended victim.  Powered by corporate databases and social networking sites, any bit of information an attacker can find will be useful. At the very minimum, most whaling attacks involve the name, job and some very basic details about the targets.

Email security firm Mimecast polled over 400 IT experts in the US, UK, South Africa and Australia in March 2016 to ask them about the state of play in the first three months of the year. Globally 67% of respondents said they saw a jump in the number of whaling incidents designed to defraud them of revenue, while 43% saw an increase in attacks looking for sensitive corporate information. The FBI warned back in February of a sharp rise in BEC (Business Email Compromise) incidents, generating as much as $2 billion over the past two years and $800m in the previous six months alone.

To Protect Employees from Whaling Email Attacks,
- Fraudulent emails should be identified and blocked using methods like pattern recognition.
- Encouraging corporate management staff to undergo information security awareness training.
- Every corporate executive should have their social media profiles locked down by implementing privacy restrictions to prevent unknown individuals from viewing what could be sensitive information.

Sources: [infosecurity-magazine](http://www.infosecurity-magazine.com/news/whaling-attacks-jump-again-in-q1/), [social-engineer](https://www.social-engineer.com/whaling-why-go-after-minnows-when-you-can-catch-a-big-phish/), [netforbeginners](http://netforbeginners.about.com/od/scamsandidentitytheft/f/What-Is-Whaling-Spear-Phishing.htm)


#### Angular JS

Angular JS is a very popular JavaScript framework. There's a very good chance you would have heard of it even if you are not a developer. Here's a bit of info without too many geeky jargons to understand what it is and why it's popular.

AngularJS was produced by a team at Google, which gives it instant credibility and assured support, and it’s in high demand by employers. It is a structural framework for dynamic web apps, the kind of web apps in which the page you're viewing changes continuously as you interact. Angular lets you use HTML as your template language and lets you extend HTML's syntax to express your application's components clearly and succinctly. Angular’ s data binding and dependency injection eliminate much of the code you would otherwise have to write. And it all happens within the browser, making it an ideal partner with any server technology.

Angular js provides the following:
- Two way binding of data - the web page automagically changes based on the data on the page. It's really awesome, wait until you see the sample at the bottom.
- Excellent control over the document
- Support for form and form validations
- Define behavior to parts of the document - For example, Click on a checkbox to reveal a form.
- make HTML reusable
- CRUD(Create, Read, Update, Delete) : Data-binding, basic templating directives, form validation, routing, deep-linking, reusable components and dependency injection.
- Unit-testing, end-to-end testing, mocks and test harnesses.

Angular simplifies application development by presenting a higher level of abstraction to the developer. Like any abstraction, it comes at a cost of flexibility. In other words, not every app is a good fit for Angular. Angular is built around the belief that declarative code is better than imperative when it comes to building UIs and wiring software components together, while imperative code is excellent for expressing business logic.

Angular is really easy to get in to and you can churn out some nifty UI in a short amount of time.

But as with any technology, Angular has it's weak points.
- Once you start with Angular it's difficult to switch to a different framework.
- Angular wants you to build your application in a particular way, but it’s not very explicit about it.
- Angular is known for having a steep learning curve and poor documentation.

I made a page with Angular to demonstrate some of the features - [Angular Dashboard](http://codepen.io/kailash-sankar/full/pyweRW/)
The dashboard does the following -  Get location from user’s IP, fetch weather info, show date & time, a form to demonstrate fast re-rendering of HTML and a ToDo list to showcase the two way data binding . The ToDo list saves data on your browser only, open from the same browser to view the items you added. All done with just 250 lines of code excluding CSS and HTML.

Sources: [angularjs](https://docs.angularjs.org/guide/introduction), [dzone](https://dzone.com/articles/10-reasons-web-developers), [infoworld](http://www.infoworld.com/article/2612801/javascript/what-s-so-special-about-google-s-angularjs.html)
